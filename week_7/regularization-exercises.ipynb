{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Discussion (Week 7): Regularization\n", "*By Jerry Lin an Camelia Simoiu*"], "execution_count": 0, "outputs": []}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["# uncomment the following line to install packages if encounter errors on Binder\n", "# install.packages(c(\"gplots\", \"glmnet\", \"ROCR\"))\n", "\n", "set.seed(125)\n", "options(digits = 3, warn=-1)\n", "library(gplots)\n", "library(tidyverse)\n", "library(glmnet)\n", "theme_set(theme_bw())"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "## Setup\n", "\n", "### Background Revisit\n", "\n", "\"Stop-and-frisk\" is a police practice of temporarily detaining, questioning, and at times searching civilians on the street for weapons and other contraband. \n", "\n", "In New York City, between 2003 and 2013, over 100,000 stops were made per year, with 685,724 people being stopped at the height of the program in 2011. The vast majority of those stopped were African-American or Latino, raising concerns of racial bias.[^biasref]\n", "\n", "[^biasref]: https://en.wikipedia.org/wiki/Stop-and-frisk_in_New_York_City\n", "\n", "\n", "We will work with a sample of stops in NYC made between 2008-2011, recorded on a  \n", "[UF-250 form](https://www.prisonlegalnews.org/news/publications/blank-uf-250-form-stop-question-and-frisk-report-worksheet-nypd-2016/).\n", "The data can be downloaded as an `Rdata` file from [here](https://5harad.com/mse125/discussions/week_6/frisk_stops.RData).\n", "Once you've downloaded the data, you can load it into your environment with the\n", "`load()` function. This will load a single data frame named `stops`"], "execution_count": 0, "outputs": []}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["# Read the data, downloaded from\n", "# https://5harad.com/mse125/discussions/week_6/frisk_stops.RData\n", "load(\"../week_6/frisk_stops.RData\")\n", "\n", "head(stops)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "For the purpose of this exercise, we will focus on a few selected columns:\n", "\n", "* Base information regarding stop\n", "    + `id`, `date`, `time`, `precinct`, `suspected_crime`\n", "* Suspect demographics:\n", "    + `suspect_sex`, `suspect_race`, `suspect_age`\n", "* Was person frisked?\n", "    + `frisked`\n", "* Was a weapon found?\n", "    + `found_weapon`\n", "\n", "\n", "### Question\n", "\n", "In last week's discussion, we fit a logistic regression to predict the\n", "likelihood of discovering a weapon given all the covariates that an officer\n", "observed prior to conducting the frisk.\n", "This week, we will focus on evaluating the model and try to improve the model's performance through regularization.\n", "\n", "Recall we are able to fit a logistic regression by running following code snippet:"], "execution_count": 0, "outputs": []}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["# Let's only look at the first 3,000 rows for faster model fitting\n", "frisked_df <- stops %>% \n", "  filter(frisked == TRUE) %>% \n", "  slice(1:3000)\n", "  \n", "risk_model <- glm(found_weapon ~ suspect_age + suspect_race + suspected_crime,\n", "                  data = frisked_df,\n", "                  family = binomial)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "In this discussion section, we will revisit the concept of model/feature\n", "selection, and try to construct models that perform and generalize\n", "well using different regularizations, specifically, lasso and ridge.\n", "\n", "\n", "## Model Selection\n", "\n", "### Bias-variance tradeoff\n", "\n", "One major concern when we are developing a model is its _generalization_\n", "performance, i.e., how well can our model make predictions on unseen,\n", "independent new data.\n", "Recall in class, the squared error on an random datapoint $x$ can be decomposed as:\n", "\n", "\\begin{align}\n", "\\mathbb{E}[(y-\\hat{r}(x))^2] \n", "  & = \\text{bias}[\\hat{r}(x)]^2 + \\text{var}[\\hat{r}(x)] + \\sigma^2 \\\\\n", "  & = \\text{bias}^2 + \\text{variance} + \\text{irreducible error}\n", "\\end{align}\n", "\n", "As our models get increasingly complex, we can usually fit the data better,\n", "but risk having excessive variance by _overfitting_; \n", "on the other hand, when we have simpler models, we may avoid overfitting,\n", "but can potentially be _underfitting_ the data, resulting in higher bias.\n", "Therefore it's critical for us to come up with model evaluation and selection\n", "methods to find models that not only perform well on the data they are trained\n", "to, but also generalize well on unseen data by balancing the tradeoff between\n", "bias and variance.\n", "\n", "\n", "### AUC\n", "\n", "One of the most widely used performance measurements for binary classification\n", "models is area under the Receiver Operating Characteristics curve (AUROC), or\n", "simply AUC.\n", "\n", "We define the following terms:\n", "\n", "* P: the number of real positive cases in the data\n", "* N: the number of real negative cases in the data\n", "* TP: true positives; the number of predicted positive cases that were real positives \n", "* TN: true negatives; the number of predicted negative cases that were real negatives \n", "* FP: false positives; the number of predicted positives that were actually negative in the data (false alarms, Type I error)\n", "* FN: false negatives; the number of predicted negatives that were actually positive in the data (Type II error) \n", "\n", "Their definitions can be illustrated using following table:\n", "\n", "<br>\n", "<style type=\"text/css\">\n", ".tg  {border-collapse:collapse;border-spacing:0; margin: auto;}\n", ".tg td{font-family:Arial, sans-serif;font-size:14px;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#BBBBBB;}\n", ".tg th{font-family:Arial, sans-serif;font-size:14px;font-weight:normal;padding:10px 5px;border-style:solid;border-width:1px;overflow:hidden;word-break:normal;border-color:#BBBBBB;}\n", ".tg .tg-baqh{text-align:center;vertical-align:top}\n", "</style>\n", "<table class=\"tg\">\n", "  <tr>\n", "    <th class=\"tg-baqh\"></th>\n", "    <th class=\"tg-baqh\">Real positive</th>\n", "    <th class=\"tg-baqh\">Real negative</th>\n", "  </tr>\n", "  <tr>\n", "    <td class=\"tg-baqh\">Predicted positive</td>\n", "    <td class=\"tg-baqh\">TP</td>\n", "    <td class=\"tg-baqh\">FP</td>\n", "  </tr>\n", "  <tr>\n", "    <td class=\"tg-baqh\">Predicted negative</td>\n", "    <td class=\"tg-baqh\">FN</td>\n", "    <td class=\"tg-baqh\">TN</td>\n", "  </tr>\n", "</table>\n", "<br>\n", "\n", "The true positive rate (TPR) is then given by:\n", "\n", "\\begin{equation}\n", "  TPR = \\frac{TP}{P} = \\frac{TP}{TP + FN}\n", "\\end{equation}\n", "\n", "And the false positive rate (FPR) is given by: \n", "\n", "\\begin{equation}\n", "  FPR = \\frac{FP}{N} = \\frac{TP}{FP + TN}\n", "\\end{equation}\n", "\n", "\n", "The ROC curve is a performance measurement for binary classification problems at\n", "various thresholds settings.\n", "The Receiver Operating Characteristics (ROC) Curve traces the percentage of \n", "true positives accurately predicted by a given model as the prediction \n", "threshold decreases from 1 to 0.\n", "For a good model, as the threshold decreases, it predict more real positives as \n", "\"positive\" and less real negatives as being positive.\n", "So for a good model, the curve should rise steeply, indicating that the TPR \n", "(vertical axis) increases faster than the FPR (horizontal axis) as the threshold \n", "decreases.\n", "Greater the area under the ROC curve, better the predictive ability of the model.\n", "\n", "AUC stands for Area Under the Curve.\n", "Under machine learning or statistical learning context, AUC often (implicitly)\n", "refers to the area under the ROC curve. Sometimes, you may also see it's\n", "referred as AUROC or ROC AUC<sup>[1](#auc_is_bad)</sup>.\n", "<a name=\"auc_is_bad_back\"></a>\n", "\n", "The AUC has several equivalent interpretations:\n", "\n", "* The expectation that a uniformly drawn random positive is ranked higher than a\n", "  uniformly drawn random negative.\n", "  \n", "* The expected proportion of positives ranked higher than a uniformly drawn \n", "  random negative.\n", "\n", "<a name=\"auc_below_50_back\"></a>\n", "\n", "AUC typically ranges between 0.5 and 1 <sup>[2](#auc_below_50)</sup>, where 0.5 suggests a model that outputs random\n", "predictions while 1 suggests a model with perfect prediction.\n", "\n", "We are able to calculate the AUC of our previously fitted mode, using the `ROCR`\n", "package, with following code:"], "execution_count": 0, "outputs": []}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["library(ROCR)\n", "\n", "# prediction and performance functions are from package ROCR\n", "compute_auc <- function(p, labels) {\n", "  pred <- prediction(p, labels)\n", "  auc <- performance(pred, 'auc')\n", "  auc <- unlist(slot(auc, 'y.values'))\n", "  auc\n", "}\n", "y_pred <- predict(risk_model, frisked_df, type=\"response\")\n", "\n", "compute_auc(y_pred, frisked_df$found_weapon)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can also visualize the ROC curve"], "execution_count": 0, "outputs": []}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["# prediction and performance functions are from package ROCR\n", "plot_roc <- function(p, labels) {\n", "  pred <- prediction(p, labels)\n", "  perf <- performance(pred,\"tpr\",\"fpr\")\n", "  plot(perf, col=\"black\")\n", "}\n", "plot_roc(y_pred, frisked_df$found_weapon)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "### Train, validate, test\n", "\n", "Selecting a model with the highest AUC on the dataset we train our model on is\n", "not going to give us the \"best\" model that both perform and generalize well.\n", "Keep in mind that we have two separate goals to achieve:\n", "\n", "1. __Model selection__: estimating the performance of different models in order\n", "to choose the best one\n", "\n", "<a name=\"two_goals_back\"></a>\n", "\n", "2. __Model assessment__: having chosen a final model, estimating its prediction\n", "error (generalization error) on new data<sup>[3](#two_goals)</sup>\n", "\n", "\n", "One good way to achieve both goals is to divide our dataset into three parts:\n", "_training set_, _validation set_, and _test set_.\n", "We fit our model using the training set; then we use a validation set to\n", "select a model that achieve the best performance (e.g., based on AUC).\n", "<!-- estimate prediction performance (e.g., evaluate AUC) for model selection; -->\n", "Finally, we use the test set to assess the generalization error of the model\n", "that we end up selecting.\n", "Note that the performance that we estimate on the validation set for our\n", "selected model will likely be an overestimate of the model's true performance;\n", "exactly because we happened to choose the model that achieved the highest\n", "performance on the validation set.\n", "Hence, we need the final step---performance evaluation on a test set---to get\n", "an unbiased estimate of the model's generalization error.\n", "<!--\n", "Note that we may go back and forth between those stages, but generally we should\n", "refrain from using the test set until we are set on the choice of final model.\n", "If we repeatedly using the test set in the process of choosing our model, we are going to underestimate the true test (generalization) error.\n", "-->\n", "\n", "<a name=\"splits_back\"></a>\n", "\n", "One way to split our data into training, validation, and test set with\n", "50%, 25%, and 25% of the data, respectively, we have<sup>[4](#splits)</sup>:"], "execution_count": 0, "outputs": []}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["set.seed(123)\n", "\n", "# shuffle the dataset first\n", "shuffled <- frisked_df[sample(nrow(frisked_df)),]\n", "\n", "\n", "# Your code here!\n", "# we should first calculate the size of our training, validation, and test size\n", "# and then use the slice function to slice the data into three pieces of corresponding\n", "# sizes: train_df, valid_df, and test_df\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Then we can fit our model on the training set"], "execution_count": 0, "outputs": []}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["# Using formula\n", "form <- formula(found_weapon ~ suspect_age + suspect_race + suspected_crime)\n", "lr <- glm(form, data = train_df, family = binomial)\n", "y_pred <- predict(lr, train_df, type=\"response\")\n", "\n", "# Training AUC\n", "format(compute_auc(y_pred, train_df$found_weapon), digits=3)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "Next, we can estimate prediction performance on our validation set.\n", "We see a significant performance drop in terms of AUC, suggesting that our\n", "logistic regression model is overfiting the training data."], "execution_count": 0, "outputs": []}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["y_pred <- predict(lr, valid_df, type=\"response\")\n", "\n", "# Validation AUC\n", "format(compute_auc(y_pred, valid_df$found_weapon), digits=3)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Finally, we examine performance on the test set\n", "(since we do not select our model here, the test set is rather trivial)"], "execution_count": 0, "outputs": []}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["y_pred <- predict(lr, test_df, type=\"response\")\n", "\n", "# test AUC\n", "format(compute_auc(y_pred, test_df$found_weapon), digits=3)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "### Cross validation\n", "\n", "When evaluating a model's performance, sometimes we may not want to limit the\n", "performance evaluation to a spcific subset of the data (e.g., the validation\n", "set).\n", "Then, we can use _k-fold cross validation_.\n", "In a k-fold cross validation, we devide the data into $k$ (randomly shuffled)\n", "subsets.\n", "For each subset, we will use the remaining $k-1$ subsets as our training set,\n", "and evaluate the model's performance on this subset. \n", "By averaging the performance over all $k$ subset, we are able to obtain a\n", "performance estimation based on all the (training and validation) data we have.\n", "Note that, when using cross validatoin, we would still like to hold out a test \n", "set for the same reason stated in the _train, validate, test_ subsection.\n", "\n", "One combination of the two model selection paradigms is to separate the dataset\n", "into training and test set, then conduct cross valiation within the training set\n", "(so that we are creating many validation sets during cross validation); \n", "and then after selecting a final model, we evaluate its generalization error on\n", "the held-out test set.\n", "\n", "\n", "### Creating simple models\n", "\n", "<a name=\"other_model_back\"></a>\n", "With these model evaluation frameworks, we can start conducting a model\n", "selection processes to decide what variables we would like to include in our\n", "logistic regression<sup>[5](#other_model)</sup>.\n", "In order to fit the training set the best, one obvious idea is to use as many as\n", "variables as possible, but that is going to increase the model complexity and\n", "potentially lead to overfitting.\n", "Additionally, too many coefficients for these variables also make it hard for us\n", "to interpret the model.\n", "We will now discuss how to create simpler models via _feature selection_ and\n", "_regularization_.\n", "\n", "\n", "\n", "## Feature Selection\n", "\n", "<a name=\"feature_vs_variable_back\"></a>\n", "One way to create simple models is to limit the number of\n", "features<sup>[6](#feature_vs_variable)</sup> while keep the functional form of the model \n", "intact.\n", "We discussed 3 ways of doing so in class.\n", "\n", "<a name=\"not_only_regression_back\"></a>\n", "The most straightfoward way to find out whether a regression models with fewer \n", "variables would perform well is to fit the regression with all possible subsets \n", "of features. \n", "This method is referred to as best subset regression<sup>[7](#not_only_regression)</sup>. \n", "However, with $k$ features, it requires fitting $2^k$ different regression\n", "models, which is computationally expensive and inefficient.\n", "\n", "Alternatively, a greedy search for the best subset called _forward stepwise\n", "regression_ can be adopted. \n", "The idea is simple: we add one feature at time, choosing the single feature that\n", "improves the performance most.\n", "Specifically:\n", "\n", "1. We start with only an intercept in the regression\n", "2. We try adding each of the unselected features to our exisiting model,\n", "   and calculate performance (e.g., AUC, accuracy, \n", "   [AIC](https://en.wikipedia.org/wiki/Akaike_information_criterion),\n", "   [BIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion)) \n", "3. Select the feature that achieves the highest performance and include that\n", "   feature into our exisiting model\n", "4. Repeat step 2 and 3 until all features are added to the model\n", "5. Select the model that performs the best\n", "\n", "Another similar approach is _backward stepwise regression_ where, instead of\n", "starting with only the intercept and incrementally adding new features, we start\n", "with the model including all features, and remove one feature at a time that\n", "reduce the performance metric the least."], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## Regularization\n", "\n", "Another way to prevend overfitting by simplifying models is to \"shrink\" \n", "coefficients via regularization.\n", "We discussed two types of regularization in class: $L^1$ regularization (lasso)\n", "and $L^2$ regularization (ridge).\n", "Regularizations are implemented by having the regression adopt different loss \n", "functions that essentially penalize big coefficients.\n", "\n", "Regularizations are particularly helpful when many features are highly\n", "correlated (see\n", "[multicollinearity](https://en.wikipedia.org/wiki/Multicollinearity)), in which \n", "case, the coefficients can be poorly determined and therefore result in high \n", "variance in our mdoel.\n", "\n", "\n", "<a name=\"see_more_glmnet_back\"></a>\n", "In practice, we can use the `glmnet` package in `R` to implement lasso or\n", "ridge<sup>[8](#see_more_glmnet)</sup>.\n", "<a name=\"elastic_net_back\"></a>\n", "Setting parameter `alpha = 1` gives us lasso regression and setting `alpha = 0` \n", "gives us ridge regression<sup>[9](#elastic_net)</sup>.\n", "The `lambda` parameter corresponds to the $\\lambda$ term in the loss function \n", "introduced in class, indicating how heavily we would like to penalize large \n", "coefficients.\n", "The larger $\\lambda$ is, the smaller and/or sparser the coefficients are likely \n", "to be."], "execution_count": 0, "outputs": []}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["set.seed(123)\n", "\n", "# Recall that we defined our model formula as form\n", "print(form)\n", "\n", "# Unlike lm(), we need to construct the design matrix and a vector of labesl\n", "# when using glmnet()\n", "x <- model.matrix(form, train_df)[,-1]\n", "y <- train_df$found_weapon\n", "lasso <- glmnet(x, y, alpha = 1, lambda = 5e-3, family = \"binomial\")\n", "ridge <- glmnet(x, y, alpha = 0, lambda = 1e-1, family = \"binomial\")\n", "x_valid <- model.matrix(form, valid_df)[,-1]\n", "y_valid <- valid_df$found_weapon\n", "\n", "# using our pre-trained unregularized logistic regression\n", "y_pred_lr <- predict(lr, valid_df, type=\"response\")\n", "# L1 regularization, or lasso\n", "y_pred_lasso <- predict(lasso, newx = x_valid, type = \"response\")\n", "# L2 regularization, or ridge\n", "y_pred_ridge <- predict(ridge, newx = x_valid, type = \"response\")\n", "\n", "#comparing performance on validation set\n", "paste(\"logistic AUC:\", format(compute_auc(y_pred_lr, y_valid), digits=3))\n", "paste(\"lasso AUC:\", format(compute_auc(y_pred_lasso, y_valid), digits=3))\n", "paste(\"ridge AUC:\", format(compute_auc(y_pred_ridge, y_valid), digits=3))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["If we compare the coefficients of lasso and ridge, we observe that dispite their similar AUCs, lasso produces sparser coefficients (i.e., many zero coefficients) while ridge on average gives smaller coefficient values."], "execution_count": 0, "outputs": []}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["# lasso coefficients\n", "print(coef(lasso))"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["# ridge coefficients\n", "print(coef(ridge))"]}, {"cell_type": "markdown", "metadata": {}, "source": ["During this particular model training and selection process, the `lambda`\n", "parameter is the one we will be tuning.\n", "Ultimately, we wish to select the value of `lambda` that achieves best\n", "performance on the validation set.\n", "\n", "Now let's see what the best model we can find that yields the highest validation AUC:"], "execution_count": 0, "outputs": []}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["# Your code here!\n", "# Train a model named \"selected_model\" that yields the highest validation AUC\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["\n", "After many rounds of testing different `lambda`s, it seems that ridge with \n", "`lambda = 0.1` performs the best.\n", "So now let's take a look at it's generalization performance by checking the test\n", "set:\n"], "execution_count": 0, "outputs": []}, {"cell_type": "code", "execution_count": 0, "metadata": {"scrolled": true}, "outputs": [], "source": ["x_test <- model.matrix(form, test_df)[,-1]\n", "y_test <- test_df$found_weapon\n", "y_test_pred_ridge <- predict(selected_model, newx = x_test, type = \"response\")\n", "format(compute_auc(y_test_pred_ridge, y_test), digits=3)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## (Optional) Advanced selection of `lambda`\n", "\n", "<a name=\"nlambda_back\"></a>\n", "\n", "In the previous section, we compared regularized models with rather arbitrary\n", "values of `lambda`.\n", "In reality, we would fit models for multiple values of `lambda`s, and use our \n", "validation set to find the \"best\" of `lambda`.\n", "The `glmnet` package actually implements an efficient method for fitting \n", "regularized models with many `lambda` values (also referred to the \n", "_regularization path_), as long as we specify the number of `lambda` values\n", "we would like to try<sup>[10](#nlambda)</sup>.\n", "As an exercise, we could repeat the previous section, but with 10 values of\n", "`lambda` for both lasso and ridge regression:"], "execution_count": 0, "outputs": []}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["# Create a list of models so we can keep track for later\n", "models <- list(logistic = lr)\n", "\n", "# First start with a unregularized logistic regression model\n", "y_pred_lr <- predict(lr, valid_df, type=\"response\")\n", "perf_df <- tibble(\n", "  model = \"logistic\",\n", "  alpha = NA,\n", "  lambda = 0,\n", "  auc =  compute_auc(y_pred_lr, y_valid)\n", ")\n", "models$lasso <- glmnet(x, y, alpha = 1, nlambda = 10, family = \"binomial\")\n", "\n", "# You can extract the (multible) values of lambda used for glmnet via $lambda\n", "# here, we add the performance of each lambda value to perf_df\n", "perf_df <- bind_rows(\n", "  perf_df, \n", "  map_dfr(models$lasso$lambda, ~ tibble(  \n", "    model = \"lasso\",  \n", "    alpha = 1,  \n", "    lambda = .x,  \n", "    auc = compute_auc( \n", "      predict(models$lasso, x_valid, s = .x),\n", "      y_valid\n", "      ))))\n", "\n", "# We can repeat the same for ridge models\n", "models$ridge <- glmnet(x, y, alpha = 0, nlambda = 10, family = \"binomial\")\n", "perf_df <- bind_rows(\n", "  perf_df, \n", "  map_dfr(models$ridge$lambda, ~ tibble(  \n", "    model = \"ridge\",\n", "    alpha = 0,  \n", "    lambda = .x,  \n", "    auc = compute_auc( \n", "      predict(models$ridge, x_valid, s = .x),  \n", "      y_valid\n", "      ))))\n", "\n", "# Then, we can sort perf_df by auc to find the \"best\" model\n", "best_model_specs <- perf_df %>% \n", "  arrange(desc(auc)) %>% \n", "  slice(1)\n", "best_model_specs"]}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["# We can evalute the performance of the \"best\" model on our test set\n", "best_model <- models[[best_model_specs$model]]\n", "pred_test <- predict(best_model, x_test, s = best_model_specs$lambda)\n", "\n", "format(compute_auc(pred_test, y_test), digits=3)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Footnote:\n", "\n", "<a name=\"auc_is_bad\">1</a>. By \"AUC\" we often mean AUROC, as in this class. \n", "However, in general, a more precise acronym is AUROC, since technically AUC \n", "could be the area under any curve.\n", "Indeed, \"AUC\" sometimes may refer to the \"Area Under the precision-recall Curve\". &nbsp;&nbsp; [\\[go back\\]](#auc_is_bad_back)\n", "\n", "<a name=\"auc_below_50\">2</a>. If you have a worse than random model (e.g., you intentionally set your prediction to be $1-p$ where $p$ is the prediction of model that performs well), the AUC can drop below 0.5. &nbsp;&nbsp; [\\[go back\\]](#auc_below_50_back)\n", "\n", "<a name=\"two_goals\">3</a>. From section \"Model Assessment and Selection\" in Trevor, Hastie,\n", "Tibshirani Robert, and Friedman JH. \"The elements of statistical learning: data\n", "mining, inference, and prediction.\" (2009). &nbsp;&nbsp; [\\[go back\\]](#two_goals_back)\n", "\n", "<a name=\"splits\">4</a>. Note that there are many ways to split a given dataset into three\n", "pieces. This decision, however, will largely depend on the size/context of the \n", "data, which is beyond the scope of our discussion. &nbsp;&nbsp; [\\[go back\\]](#splits_back)\n", "\n", "\n", "<a name=\"other_model\">5</a>. The process of model selection can involve both the selection of\n", "variables (features) and their corresponding parameters and the selection of\n", "different models (e.g., SVM, Gaussian processes, neural networks). \n", "In this discussion, we will focusing on selection of features and model\n", "parameters. &nbsp;&nbsp; [\\[go back\\]](#other_model_back)\n", "\n", "<a name=\"feature_vs_variable\">6</a>. In this section, we use the word feature and variable\n", "interchangeably.\n", "They both refer to the covariates we include in our logistic regression model. &nbsp;&nbsp; [\\[go back\\]](#feature_vs_variable_back)\n", "\n", "<a name=\"not_only_regression\">7</a>. Although the feature selection methods we mention ended \n", "with \"regression\", they can easily be adapted for use with other statistical or\n", "machine learning models. &nbsp;&nbsp; [\\[go back\\]](#not_only_regression_back)\n", "\n", "<a name=\"see_more_glmnet\">8</a>. As always, learn more about `glmnet` by typing `?glmnet` in\n", "`R` &nbsp;&nbsp; [\\[go back\\]](#see_more_glmnet_back)\n", "\n", "<a name=\"elastic_net\">9</a>. Setting `alpha` to be somewhere between 0 and 1, on the other\n", "hand, results in [elastic net\n", "regularization](https://en.wikipedia.org/wiki/Elastic_net_regularization). &nbsp;&nbsp; [\\[go back\\]](#elastic_net_back)\n", "\n", "<a name=\"nlambda\">10</a>. Indeed, if you read the documentation for `glmnet` carefully, \n", "you'll find that manually specifying a single value value of `lambda` is\n", "actually _discouraged_!\n", "We just do that in this class to keep things a little (conceptually) simpler.\n", " &nbsp;&nbsp; [\\[go back\\]](#nlambda_back)\n", "\n"], "execution_count": 0, "outputs": []}], "metadata": {"kernelspec": {"display_name": "R", "language": "R", "name": "ir"}, "language_info": {"codemirror_mode": "r", "file_extension": ".r", "mimetype": "text/x-r-source", "name": "R", "pygments_lexer": "r", "version": "3.6.3"}}, "nbformat": 4, "nbformat_minor": 4}