{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Week 5 Discussion: Linear Regression"], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["*Originally by Jerry Lin.*\n", "*Minor edits from Alex Chohlas-Wood.*"], "execution_count": 0, "outputs": []}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["# Setup \n", "library(tidyverse)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Background\n", "\n", "We all love learning, right? Well... at the very least, we care about the grades. \n", "\n", "In this discussion session, we, as a group of applied statisticians who are constantly thinking about real-world high-impact applications, are going to explore if we can predict the final exam scores using homework scores using a synthetic (but realistic enough) dataset.\n", "\n", "After all, grades are all that matters (kidding)."], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## Dataset\n", "\n", "The dataset contains scores of five homework assignments and a final exam \n", "for `r nrow(data)` (hypothetical) students who took a statistics class from \n", "last year.\n", "Even though it is early-ish in this quarter and we've only received feedback for the first 3 homeworks for this (hypothetical) class we are taking, we'd like to build a model to predict our final exam scores in this class using only the first few homework scores, so that we are able to forecast our (good) grades early, and enjoy peace of mind.\n", "\n", "Specifically, suppose we have observed our first 3 homework scores as 100, 85, and 95, can we predict our final exam scores?\n", "\n", "To answer this question, we will start by analyzing the data and fitting a simple regression model using only the score from hw1.\n", "Then we will fit another model using all 3 homework scores, and examine the model closely.\n", "\n", "First, let's load the data and plot hw1 scores and final exam scores as a scatterplot."], "execution_count": 0, "outputs": []}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["data <- read_csv(\"scores.csv\")\n", "\n", "head(data)\n", "\n", "# Your code here! \n", "# Make a scatterplot, with hw1 scores on the x-axis, and final test scores on the y-axis.\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["In the plot, it seems like there is a positive relationship between `hw1` \n", "and final exam scores. \n", "Now, we would like to quantitatively measure this (linear) relationship between\n", "the two variables."], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Correlation\n", "\n", "To achieve this, we define the correlation coefficient $\\rho_{X,Y}$ between two random variable $X$ and $Y$ to be\n", "\n", "\\begin{align}\n", "\\rho_{X,Y} &= \\frac{\\mathrm{cov}(X, Y)}{\\sigma_X\\sigma_Y} \\\\ \n", "  & = \\mathbb{E} \\left[ \\frac{(X-\\mu_X)}{\\sigma_X} \n", "  \\frac{(Y-\\mu_Y)}{\\sigma_Y} \\right]\n", "\\end{align}\n", "\n", "In other words, the definition above shows that we can think of the correlation as the covariance of $X$ and $Y$\n", "scaled by the individual variance of $X$ and $Y$.\n", "\n", "The correlation coefficient ranges from -1 to +1, where -1 indicates a perfect\n", "downhill (negative) linear relationship and +1 indicates a perfect uphill\n", "(positive) linear relationship.\n"], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["To illustrate this, the following plot shows how different values of correlation $\\rho$ appear in a scatterplot:"], "execution_count": 0, "outputs": []}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["rhos <- sort(c(0.99, -.85, .6, .15))\n", "\n", "get_by_rho <- function(rho, n = 50) {\n", "  X1 <- runif(n)\n", "  X2 <- runif(n)\n", "  Y <- rho*X1 + sqrt(1 - rho^2)*X2\n", "  tibble(X = X1 - mean(X1), Y = Y - mean(Y))\n", "}\n", "d <- tibble(rho = rhos) %>% \n", "  mutate(data = map(rho, get_by_rho)) %>% \n", "  unnest(data)\n", "  \n", "ggplot(d, aes(x = X, y = Y)) +\n", "  geom_abline(aes(intercept = 0, slope = rho), linetype = \"dashed\") +\n", "  geom_point() +\n", "  facet_wrap(~ rho, scales = \"free\", labeller = label_bquote(rho == .(rho))) +\n", "  theme(\n", "    axis.title = element_blank(),\n", "    axis.ticks = element_blank(),\n", "    axis.text = element_blank()\n", "    )"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Empirically, the sample correlation between $X$ and $Y$ from a dataset can be estimated as:\n", "\n", "$$\n", "r_{X,Y} = \\frac{1}{n}\\sum_{i=1}^{n}\\frac{X_i - \\bar{X}}{\\hat{\\sigma}_X}\\frac{Y_i - \\bar{Y}}{\\hat{\\sigma}_Y}\n", "$$\n", "\n", "Which might look a little painful, but lucky for us, `R` comes with a `cor()`\n", "function which calculates the correlation coefficient between two vectors.\n", "\n", "Going back to our original example, we can calculate the correlation coefficient\n", "for `hw1` scores and `final` scores as"], "execution_count": 0, "outputs": []}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["# Your code here!\n", "# Use R's cor() function to calculate the correlation between the hw1 column and the final scores column.\n", "\n", "# HINT: Use R's built-in $ method for accessing columns. \n", "# For example, data$hw2 returns just the values in column hw2.\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["... which suggests a fairly strong positive correlation."], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Linear Regression\n", "\n", "Now, let's model this relationship between `hw1` and final scores using \n", "simple linear regression.\n", "Recall that simple linear regression is just a model with a\n", "single explanatory variable, where we are trying to find the \"best fitting\" line\n", "through the points to represent the relationship between the covariate (or the predictor) and the outcome (or the response)."], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## Review\n", "\n", "Under the framework of simple linear regression, we assume that the true \n", "underlying relationship between $X$ and $Y$ is\n", "\n", "$$Y = \\beta_0 + \\beta_1 X + \\varepsilon \\quad \\textrm{where} \\quad\n", "\\varepsilon \\sim N(0, \\sigma^2).$$\n", "\n", "Notice that in this model, the randomness is in $\\varepsilon$,\n", "which we are using to represent the *error term*, which captures\n", "unobserved factors and/or random variations that influence the value of $Y$.\n", "Due to this unobserved randomness, our linear model will not perfectly predict every possible data point.\n", "Instead, what we predict is the *average* value of $Y$ given the observed $X$.\n", "\n", "$$\n", "\\hat{Y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 X\n", "$$\n", "\n", "Within the observed data, $\\hat{\\beta}_1$ indicates *on average* how much \n", "$Y$ increased when $X$ increased by 1 unit;\n", "$\\hat{\\beta}_0$ indicates the average value of $Y$ when $X = 0$. \n", "\n", "<!-- Hence, we can rewrite the model in terms of $\\varepsilon$\n", "\n", "$$\n", "\\varepsilon = \n", "Y - \\beta_0 + \\beta_1 X\n", "$$\n", "\n", "where $Y$ and $X$ are the observed data, and the regression coefficients \n", "$\\beta_0$ and $\\beta_1$ are the parameters we wish to estimate.\n", "We then estimate $\\varepsilon$ via the empirical counterpart of the error,\n", "the *residual*:\n", "\n", "$$\n", "\\hat{\\varepsilon} = Y - \\hat{\\beta}_0 + \\hat{\\beta}_1 X\n", "$$ -->\n"], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["## Fitting a simple linear regression model\n", "\n", "Now, let's fit a simple linear model on our dataset:"], "execution_count": 0, "outputs": []}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["# Your code here!\n", "# Fit a simple linear regression predicting final scores using hw1 scores.\n", "\n", "# HINT: for simple linear regression, use the following syntax:\n", "# mod <- lm(y ~ x, data = df)\n", "# Where you replace x and y with your column names, and df with your dataframe\n", "# Then type summary(mod) to see the summary of your model.\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We observe that $\\hat{\\beta}_0 = 9.1$ and\n", "$\\hat{\\beta}_1 = 0.82$.\n", "This suggests that on average when a student's `hw1` score improved by 10 units,\n", "the final exam score improved by 8.2 points."], "execution_count": 0, "outputs": []}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["# Your code here!\n", "# Add the simple linear regression line to your scatterplot.\n", "\n", "# HINT: use ggplot's built-in geom_smooth() function to plot the line as follows:\n", "# geom_smooth(formula = y ~ x, method = \"lm\", se = FALSE) \n", "# (No need to replace variable names this time)\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Multiple regression\n", "\n", "Now, let's expand our regression to include all three homework scores."], "execution_count": 0, "outputs": []}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["# Your code here!\n", "# Fit a multiple linear regression predicting final test scores using hw1, hw2, and hw3 scores.\n", "\n", "# HINT: Use the following syntax:\n", "# multi_mod <- lm(y ~ a + b + c, data = df)\n", "# Then use summary() like we did above to inspect the results.\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The coefficients here\u2014say, for `hw2`\u2014suggest that, holding `hw1` and `hw3`\n", "scores fixed, for every 10 point improvement on `hw2`, we would\n", "expect a 2.8 point increase in the final exam on average.\n", "\n", "Note that the estimated value of $\\beta$ is different for each homework,\n", "suggesting different levels of association between homework\n", "scores and the final exam score.\n", "This could be due to a variety of reasons.  For example, it's possible that questions on the final exam focus more on the \n", "knowledge covered in the first homework. \n", "It's difficult to conclude.\n", "We would have to examine the data closely with more background knowledge \n", "regarding the data in order to answer this question."], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Uncertainty in Regression\n", "\n", "Recall from [lecture 3](https://docs.google.com/presentation/d/1F0_bFO93eNc6dQ9gnVwwu88oOyxFipl0w7tsYVcmg3E/edit?usp=sharing) \n", "(intro to statistical inference) that there is randomness in our estimator due\n", "to the fact that we only get to observe samples of the population, but never the\n", "whole population (remember the \"sampling distribution\").\n", "We have to deal with similar uncertainty here. While we may be able to find the \"best\" coefficients\n", "that fit our data, we need to also measure how (un)certain we are about our\n", "estimated coefficient, and thereby the resulting prediction.\n", "\n", "We can consider the $\\hat{\\beta}$ coefficients to be random variables normally\n", "distributed around the true $\\beta$, where the randomness comes from the sample\n", "on which we happen to fit our model.\n", "We are able to see the standard error of our $\\beta$ estimators from the \n", "regression summary table below."], "execution_count": 0, "outputs": []}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["summary(multi_mod)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can use *confidence intervals* and *prediction intervals* to quantify\n", "(un)certainty in our estimates and the resulting predictions.\n", "A confidence interval tells us how sure we are about the mean prediction given\n", "$X$, while a prediction interval tells quantifies how confident we are about a\n", "specific prediction given $X$.\n", "The following plot shows the confidence interval (the shaded area around the\n", "blue regression line) and the prediction interval (the dashed red bounds).$^1$"], "execution_count": 0, "outputs": []}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["# Confidence interval and prediction interval for our simple linear regression model.\n", "\n", "# We use a simple linear regression model for the ease of visualization only,\n", "# but we can similarly calculate the confidence interval and \n", "# prediction interval for regression models with multiple covariates.\n", "ci <- predict(mod, interval = \"prediction\")\n", "\n", "data_with_cis <- cbind(data, ci)\n", "\n", "data_with_cis %>%\n", "    ggplot(aes(x = hw1, y = final)) +\n", "      geom_point() +\n", "      geom_line(aes(y = lwr), color = \"red\", linetype = \"dashed\") +\n", "      geom_line(aes(y = upr), color = \"red\", linetype = \"dashed\") +\n", "      geom_smooth(method = \"lm\", color = \"blue\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Similar to what we observe between standard errors of the mean estimator and the\n", "sample standard deviation, the confidence interval (about the mean) is much\n", "narrower than the prediction interval.\n", "\n", "Now, suppose that we received an 80 on `hw1`. \n", "We are able to produce our confidence interval and prediction interval using `predict`.$^2$"], "execution_count": 0, "outputs": []}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["# Confidence interval\n", "predict(mod,\n", "        newdata = tibble(hw1 = 80),\n", "        interval = \"confidence\",\n", "        level = .95)\n", "\n", "# Prediction interval\n", "predict(mod,\n", "        newdata = tibble(hw1 = 80),\n", "        interval = \"prediction\",\n", "        level = .95)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Goodness of Fit\n", "\n", "Now we have a working simple linear regression model and are able to quantify\n", "the the uncertainty in the model.\n", "But we would still like to know how well our model fits the data overall.\n", "\n", "Recall when fitting our regression on a specific dataset, we aim to minimize the\n", "*sum of squared residuals* (or *sum of squared errors (SSE)*):\n", "\\begin{align}\n", "  \\sum_{i=1}^{n} \\hat{\\varepsilon}_i^2 \n", "    & = \\sum_{i=1}^{n} (Y_i - \\hat{Y}_i) ^ 2  \\\\\n", "    & = \\sum_{i=1}^{n} \\big(Y_i - (\\hat{\\beta}_0 + \\hat{\\beta}_1 X_i)\\big)^2.\n", "\\end{align}\n", "\n", "Then we are able to estimate the *root mean square error (RMSE)* or the\n", "*residual standard error* as \n", "\n", "$$\n", "\\hat{\\varepsilon}_i =\n", "\\sqrt{\\frac{1}{n}\\sum_{i=1}^{n} (Y_i - \\hat{Y}_i) ^ 2}.\n", "$$ \n", "\n", "The lower it is, the better---since the goal is to minimize the residuals.\n", "\n", "Another common metric for goodness of fit is $R^2$, defined as:\n", "\n", "$$\n", "R^2 = 1 - \\frac{\\sum_{i=1}^{n}(Y_i - \\hat{Y}_i)^2}\n", "               {\\sum_{i=1}^{n}(Y_i - \\bar{Y})^2}.\n", "$$\n", "\n", "This metric measures how good the full model (with all covariates) is\n", "compared to the reduced model (with an intercept only), or, how much of the\n", "variation in the dataset can be explained compared to the total variation.\n", "$R^2$ ranges from 0 to 1. In general, the higher $R^2$ is, the better the model\n", "fits the data."], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["Both RMSE and $R^2$ can be found in the regression summary table by calling:"], "execution_count": 0, "outputs": []}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["summary(mod)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["$R^2$ is represented by \"Multiple R-squared\" (i.e., 0.3552) in the table above.\n", "**RMSE** is shown as \"Residual standard error\" (i.e., 11.5).\n", "\n", "Can we do better given that we have data for all five homeworks?\n", "Though including all five homework scores will (hopefully) best predict the final\n", "exam scores, we would like to make prediction as early as possible.\n", "Now let's look at the summary table of the multi regression model."], "execution_count": 0, "outputs": []}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["summary(multi_mod)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["From the summary table above, both $R^2$ (0.4488) and **RMSE** (10.84) indicate that our multi\n", "linear regression fits the data better than the simple linear regression.\n", "\n", "The following code visualizes how the fit changes if we include the first $k$ \n", "homework scores in the model as $k$ increases."], "execution_count": 0, "outputs": []}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["eval_perf <- function(k, data) {\n", "  # This function fits a model with the first k columns of data\n", "  form <- reformulate(colnames(data)[1:k], response = \"final\")\n", "  my_mod <- lm(form, data = data)\n", "  \n", "  R2 <- summary(my_mod)$r.squared\n", "  RMSE <- summary(my_mod)$sigma\n", "  # RMSE with degree of freedom n-k-1, but let's not get into that rabbit hole here...\n", "  # sqrt(c(crossprod(my_mod$residuals)) / (length(my_mod$residuals) - (k + 1)))\n", "  c(k, R2, RMSE)\n", "}\n", "perf <- as_tibble(t(sapply(1:5, function(k)\n", "eval_perf(k, data))))\n", "colnames(perf) <- c(\"k\", \"R2\", \"RMSE\")\n", "# Number of homework scores vs. R2\n", "ggplot(data = perf, aes(x = k, y = R2)) +\n", "  geom_line()\n", "# Number of homework scores vs. RMSE\n", "ggplot(data = perf, aes(x = k, y = RMSE)) +\n", "  geom_line()"]}, {"cell_type": "markdown", "metadata": {}, "source": ["It seems like we can achieve almost the best performance after observing the\n", "first 3 homework scores as the $R^2$ and RMSE curve plateaus when $k >= 3$,\n", "i.e., including additional covariates does not substantialy improve the \n", "goodness of fit."], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Final prediction \n", "\n", "Now, let's go back to the original question we were trying to answer:\n", "when we have observed our first 3 homework scores as **100, 85, and 95,** can we\n", "predict our final exam scores?\n", "\n", "Well, by plugging in those numbers in our multiple regression model, we get:"], "execution_count": 0, "outputs": []}, {"cell_type": "code", "execution_count": 0, "metadata": {}, "outputs": [], "source": ["# Your code here! \n", "# Use predict() with your multiple linear regression model \n", "# to predict the final test score, given the three hw scores listed above.\n", "\n", "# HINT: You can use the following syntax:\n", "# predict(mod, newdata = tibble(a = 1, b = 2, c = 3))\n", "# Replace mod and update the tibble with correct columns / values.\n", "\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["On average, we expect our final exam score to be 91.16.\n", "\n", "Finally, we successfully built and evaluated our final exam scores prediction\n", "model. Knowing the predicted final exam scores, now we can happily sit back and\n", "sip our coffee. The sky is bluer than ever before ... or is it?"], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Discussion\n", "\n", "* Nothing we discussed here involves proving causation.\n", "Is that sufficient for predicting a student's final score?\n", "If so, what are other situations where we do need to make causal claims?\n", "(We will talk about causal inference later in this quarter!)\n", "* Are you always better off if you have a higher $R^2$ (or RMSE)?\n", "If not, how can we better evaluate our model?\n", "* In our final prediction, if we calculate the prediction interval, \n", "the upper bound is actually greater than 100, the max possible final score.\n", "How should we interpret the prediction interval in that case?"], "execution_count": 0, "outputs": []}, {"cell_type": "markdown", "metadata": {}, "source": ["# Footnotes\n", "\n", "1. The confidence interval and prediction interval shown in the plot is\n", "based on the simple linear regression model (one covariate) as it's hard to\n", "visualize how these intervals change as a function of covariates when there is\n", "more than one covariate.\n", "1. In [HW5](https://5harad.com/mse125/#hw5) we ask you to use `predict.lm`, which is the same as using `predict()` with model fit from `lm()`.\n", "When you use `predict()` with a model that was fit from `lm()`, `R` is smart\n", "enough to actually call `predict.lm()`. So it's also fine to just use `predict()` for the homework."], "execution_count": 0, "outputs": []}], "metadata": {"kernelspec": {"display_name": "R", "language": "R", "name": "ir"}, "language_info": {"codemirror_mode": "r", "file_extension": ".r", "mimetype": "text/x-r-source", "name": "R", "pygments_lexer": "r", "version": "3.6.3"}}, "nbformat": 4, "nbformat_minor": 4}